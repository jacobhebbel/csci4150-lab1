{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobhebbel/csci4150-lab1/blob/main/Projects_lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "371ac73f"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "### Motivation / Intended Use\n",
        "This dataset is commonly used for predicting whether an individual's income exceeds 50K USD per year based on demographic and employment-related features.\n",
        "\n",
        "This dataset contains 48,000 rows, so it has over 2,000 rows. It contains 14 features, however some will not be used. We will still consider at least 10. Finally, there are several opportunities for encountering data leakage.\n",
        "\n",
        "First, a feature fnl_wgt computes how many rows in the dataset are like this. Frequency of a specific sample considers every sample in the set, which means this feature is not individual to the sample. Therefore, we will not consider this column during training.\n",
        "\n",
        "Second, there are several duplicate entries in the dataset. We must make each sample singular to give each sample equal weighting.\n",
        "\n",
        "Finally, there are two columns pertaining to education: a string representation and an enumeration of the string options. Both should not be used, as it may give that feature unequal weighting, and samples with unknown education backgrounds would be minimized.\n",
        "\n",
        "Therefore, this dataset is compliant with the project outlines.\n",
        "\n",
        "### Target Definition\n",
        "The target variable is income, a binary classification task. The two classes are `<=50K` (income is less than or equal to 50,000 USD per year) and `>50K` (income is greater than 50,000 USD per year).\n",
        "\n",
        "### Data Source + License/Terms\n",
        "*   **Data Source:** UCI Machine Learning Repository\n",
        "*   **Link:** [https://archive.ics.uci.edu/dataset/2/adult](https://archive.ics.uci.edu/dataset/2/adult)\n",
        "*   **Terms:** The dataset is publicly available for research purposes.\n",
        "\n",
        "### Feature Dictionary\n",
        "Here are some key features:\n",
        "*   `age`: continuous. The age of the individual.\n",
        "*   `workclass`: categorical. Type of employer (e.g., Private, Self-emp-not-inc, Federal-gov).\n",
        "*   `education`: categorical. The highest level of education achieved (e.g., Bachelors, HS-grad, Some-college).\n",
        "*   `marital-status`: categorical. Marital status.\n",
        "*   `occupation`: categorical. The individual's occupation (e.g., Tech-support, Craft-repair, Other-service).\n",
        "*   `race`: categorical. Race of the individual.\n",
        "*   `sex`: categorical. Gender of the individual (Male, Female).\n",
        "*   `capital-gain`: continuous. Capital gains for the individual.\n",
        "*   `capital-loss`: continuous. Capital losses for the individual.\n",
        "*   `hours-per-week`: continuous. The number of hours worked per week.\n",
        "*   `native-country`: categorical. Country of origin.\n",
        "\n",
        "### Limitations/Risks\n",
        "*   **Selection Bias:** The dataset originates from a specific census year (1994) and may not be representative of the current population.\n",
        "*   **Representativeness:** The dataset heavily samples individuals from the United States, which could limit the generalizability of models trained on this data to other geographical regions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c951952"
      },
      "source": [
        "## Data Quality Audit\n",
        "\n",
        "### Missingness Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-01-29T15:17:52.522077Z",
          "start_time": "2026-01-29T15:17:49.226409Z"
        },
        "collapsed": true,
        "id": "initial_id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3588c4d6-4c60-47e1-de78-0a9b7309f5ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.12/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2026.1.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ucimlrepo\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# fetch dataset\n",
        "adult = fetch_ucirepo(id=2)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = adult.data.features\n",
        "y = adult.data.targets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "875314fd",
        "outputId": "3a18d1b8-96c0-4d63-b3f9-b1e89c166314"
      },
      "source": [
        "missing_X = X.isnull().sum()\n",
        "missing_X = missing_X[missing_X > 0].sort_values(ascending=False)\n",
        "print('Missing values in features (X):')\n",
        "print(missing_X)\n",
        "\n",
        "missing_y = y.isnull().sum()\n",
        "missing_y = missing_y[missing_y > 0].sort_values(ascending=False)\n",
        "print('\\nMissing values in target (y):')\n",
        "print(missing_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in features (X):\n",
            "occupation        966\n",
            "workclass         963\n",
            "native-country    274\n",
            "dtype: int64\n",
            "\n",
            "Missing values in target (y):\n",
            "Series([], dtype: int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e00c6846"
      },
      "source": [
        "The `workclass`, `occupation`, and `native-country` columns in the features (`X`) have missing values, indicated by `?` in the original dataset in which we will treat as new unknown value during preprocessing. The `target` (`y`) does not have any explicit missing values.\n",
        "\n",
        "### Duplicate Row Check\n",
        "Duplicate rows were identified and removed. Additionally, the columns `fnlwgt` and `education-num` were dropped from `X` as they were features that would cause leaks.\n",
        "\n",
        "### Target Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ffded38",
        "outputId": "b734a110-60fe-46c8-eb8e-583c7b609f92"
      },
      "source": [
        "target_distribution = y.value_counts(normalize=True) * 100\n",
        "print('Target variable (income) distribution:')\n",
        "print(target_distribution)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target variable (income) distribution:\n",
            "income\n",
            "<=50K     50.612178\n",
            "<=50K.    25.459645\n",
            ">50K      16.053806\n",
            ">50K.      7.874370\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eab6303"
      },
      "source": [
        "The target distribution shows an imbalance, with the majority of individuals (`~75%`) earning `<=50K` and a smaller proportion (`~25%`) earning `>50K`. This class imbalance should be considered during model training and evaluation, as models might tend to predict the majority class more often.\n",
        "\n",
        "### One Bias/Ethics Note\n",
        "This dataset contains demographic information such as `sex`, `race`, and `native-country`. Models trained on such data could inadvertently learn biases present in the training data, leading to unfair predictions for certain demographic groups."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert X, y into correct values\n",
        "y = y.replace('<=50K.', '<=50K').replace('>50K.', '>50K')\n",
        "X = X.replace('?', np.nan)\n",
        "X = X.fillna('Unknown')\n",
        "\n",
        "# Drop leaky features\n",
        "X_for_dedupe = X.drop(columns=['fnlwgt', 'education-num'])\n",
        "\n",
        "# Boolean mask for unique rows\n",
        "mask = ~X_for_dedupe.duplicated()\n",
        "\n",
        "# Apply mask to both X and y to remove the duplicate rows\n",
        "X = X.loc[mask].reset_index(drop=True)\n",
        "y = y.loc[mask].reset_index(drop=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RR_OxN4hqQsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=0, stratify=y\n",
        ")\n",
        "\n",
        "# columns\n",
        "num_cols = [\"age\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
        "cat_cols = [\n",
        "    \"workclass\", \"education\", \"marital-status\", \"occupation\", \"race\", \"sex\", \"native-country\"\n",
        "]\n",
        "\n",
        "# preprocess\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "        ]), num_cols),\n",
        "        (\"cat\", Pipeline([\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "        ]), cat_cols),\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "# model\n",
        "model = Pipeline([\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=25, weights=\"distance\", p=1)),\n",
        "])\n",
        "\n",
        "# model results\n",
        "model.fit(X_train, y_train)\n",
        "pred = model.predict(X_test)\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U615T3Ueqb3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5d6006-2fe2-4359-fb29-73b2265e214d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neighbors/_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.8448359073359073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "labels = np.unique(y_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, pred, labels=labels)\n",
        "\n",
        "cm_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=[f\"True {l}\" for l in labels],\n",
        "    columns=[f\"Pred {l}\" for l in labels]\n",
        ")\n",
        "\n",
        "print(cm_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iJJFznA1plm",
        "outputId": "cce9cd75-c8c8-46e2-9dba-f720fefe1401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Pred <=50K  Pred >50K\n",
            "True <=50K        5823        481\n",
            "True >50K          805       1179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear SVC Model"
      ],
      "metadata": {
        "id": "NvrfMyQpYPCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# model\n",
        "model = Pipeline([\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"svm\", LinearSVC())\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    \"svm__C\": [0.01, 0.1, 1, 10],\n",
        "    \"svm__class_weight\": [None, \"balanced\"]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    model,\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# model results\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "206f5c07-e61e-4a9e-8a15-097e208b52cd",
        "id": "C2tRDLPOhn2P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'svm__C': 0.1, 'svm__class_weight': None}\n",
            "Test accuracy: 0.855815637065637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results\n",
        "\n",
        "Using a seed of random_state=0 for both logistic and SVM models, we obtained the following accuracy results:\n",
        "\n",
        "Logistic Regression: 0.8448\n",
        "\n",
        "Linear SVC: 0.8558\n",
        "\n",
        "For Linear SVC we compared different regularization values and found that C=0.1, class_weight=None was the best performing after running grid search."
      ],
      "metadata": {
        "id": "bmrV8d_IcAt2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calibrated Classifier Cross Validation with Sigmoid (Platt Scaling)\n"
      ],
      "metadata": {
        "id": "qXMDzRu_a8b5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Make sure y is binary 0/1 (adjust mapping to your labels)\n",
        "y_train_bin = (y_train.iloc[:, 0] == \">50K\").astype(int)\n",
        "y_test_bin  = (y_test.iloc[:, 0]  == \">50K\").astype(int)\n",
        "\n",
        "calibrated_clf = CalibratedClassifierCV(\n",
        "    estimator=model,     # your Pipeline (preprocess + LinearSVC)\n",
        "    method=\"sigmoid\",\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "calibrated_clf.fit(X_train, y_train_bin)\n",
        "\n",
        "# Probabilities for positive class\n",
        "probs = calibrated_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Cost-sensitive threshold (your formula gives 1/11 â‰ˆ 0.091)\n",
        "C_FP = 1\n",
        "C_FN = 1\n",
        "threshold = 1 / (C_FP + C_FN)\n",
        "\n",
        "\n",
        "y_pred_cost_sensitive = (probs >= threshold).astype(int)\n",
        "\n",
        "print(\"Cost-sensitive accuracy:\", accuracy_score(y_test_bin, y_pred_cost_sensitive))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test_bin, y_pred_cost_sensitive))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0tDPnvea8_3",
        "outputId": "055301bd-1cb4-4d4c-a7fc-3196bbd25d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost-sensitive accuracy: 0.8560569498069498\n",
            "Confusion matrix:\n",
            " [[5914  390]\n",
            " [ 803 1181]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Threshold\n",
        "thresholds = np.linspace(0.001, 0.999, 1000)\n",
        "costs = []\n",
        "\n",
        "def expected_cost(y_true, y_pred, C_FP, C_FN):\n",
        "    FP = ((y_true == 0) & (y_pred == 1)).sum()\n",
        "    FN = ((y_true == 1) & (y_pred == 0)).sum()\n",
        "    return FP * C_FP + FN * C_FN\n",
        "\n",
        "for t in thresholds:\n",
        "    preds = (probs >= t).astype(int)\n",
        "    cost = expected_cost(y_test_bin, preds, C_FP, C_FN)\n",
        "    costs.append(cost)\n",
        "\n",
        "best_idx = np.argmin(costs)\n",
        "best_threshold = thresholds[best_idx]\n",
        "\n",
        "print(\"Best threshold:\", best_threshold)\n",
        "print(\"Validation cost:\", costs[best_idx])\n",
        "\n",
        "y_pred_cost_sensitive = (probs >= best_threshold).astype(int)\n",
        "\n",
        "print(\"Cost-sensitive accuracy:\", accuracy_score(y_test_bin, y_pred_cost_sensitive))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test_bin, y_pred_cost_sensitive))"
      ],
      "metadata": {
        "id": "pdnryl-wfNIQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34abc99c-c31b-4c93-fa9a-18c000107ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best threshold: 0.49450550550550554\n",
            "Validation cost: 1182\n",
            "Cost-sensitive accuracy: 0.8573841698841699\n",
            "Confusion matrix:\n",
            " [[5907  397]\n",
            " [ 785 1199]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Threshold Results\n",
        "For our problem set, we decided on equal costs for false positives (C_FP=1) and false negatives (C_FN=1) giving us a defualt threshold of .500. We saw no reason to value one over the other. With this we did a threshold sweep and found that the best threshold was approximately 0.495. This slightly improved our accuracy score from 0.855 to 0.857.\n",
        "\n",
        "# Confusion Matrices\n",
        " Defualt:\n",
        "\n",
        " [[5914  390]\n",
        "\n",
        " [ 803 1181]]\n",
        "\n",
        "Best:\n",
        "\n",
        "  [[5907  397]\n",
        "\n",
        " [ 785 1199]]"
      ],
      "metadata": {
        "id": "ZMcXfOL_maoo"
      }
    }
  ]
}